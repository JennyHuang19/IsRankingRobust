# Robustness of LLM Ranking Systems

This repository contains code for our ICML 2025 submission to the Workshop on Models of Human Feedback for AI Alignment:  
**"Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings"**

We introduce a fast method for checking whether the rankings of top-performing large language models (LLMs) are stable to the removal of a small number of preference evaluations, focusing on Bradleyâ€“Terry (BT) based systems like Chatbot Arena and MT-Bench.
